var documenterSearchIndex = {"docs":
[{"location":"class02/overview/#Class-2-—-08/29/2025","page":"Class 2","title":"Class 2 — 08/29/2025","text":"","category":"section"},{"location":"class02/overview/","page":"Class 2","title":"Class 2","text":"Presenter: Arnaud Deza","category":"page"},{"location":"class02/overview/","page":"Class 2","title":"Class 2","text":"Topic: Numerical optimization for control (gradient/SQP/QP); ALM vs. interior-point vs. penalty methods","category":"page"},{"location":"class02/overview/","page":"Class 2","title":"Class 2","text":"Pluto Notebook for all the chapter: Here is the actual final chapter","category":"page"},{"location":"class02/overview/","page":"Class 2","title":"Class 2","text":"","category":"page"},{"location":"class02/overview/#Overview","page":"Class 2","title":"Overview","text":"","category":"section"},{"location":"class02/overview/","page":"Class 2","title":"Class 2","text":"This class covers the fundamental numerical optimization techniques essential for optimal control problems. We explore gradient-based methods, Sequential Quadratic Programming (SQP), and various approaches to handling constraints including Augmented Lagrangian Methods (ALM), interior-point methods, and penalty methods.","category":"page"},{"location":"class02/overview/#Learning-Objectives","page":"Class 2","title":"Learning Objectives","text":"","category":"section"},{"location":"class02/overview/","page":"Class 2","title":"Class 2","text":"By the end of this class, students will be able to:","category":"page"},{"location":"class02/overview/","page":"Class 2","title":"Class 2","text":"Understand the mathematical foundations of gradient-based optimization\nImplement Newton's method for unconstrained minimization\nApply root-finding techniques for implicit integration schemes\nSolve equality-constrained optimization problems using Lagrange multipliers\nCompare and contrast different constraint handling methods (ALM, interior-point, penalty)\nImplement Sequential Quadratic Programming (SQP) for nonlinear optimization","category":"page"},{"location":"class02/overview/#Prerequisites","page":"Class 2","title":"Prerequisites","text":"","category":"section"},{"location":"class02/overview/","page":"Class 2","title":"Class 2","text":"Solid understanding of linear algebra and calculus\nFamiliarity with Julia programming\nBasic knowledge of differential equations\nUnderstanding of optimization concepts from Class 1","category":"page"},{"location":"class02/overview/#Materials","page":"Class 2","title":"Materials","text":"","category":"section"},{"location":"class02/overview/#Interactive-Notebooks","page":"Class 2","title":"Interactive Notebooks","text":"","category":"section"},{"location":"class02/overview/","page":"Class 2","title":"Class 2","text":"The class is structured around four interactive Jupyter notebooks that build upon each other:","category":"page"},{"location":"class02/overview/","page":"Class 2","title":"Class 2","text":"Part 1a: Root Finding & Backward Euler\nRoot-finding algorithms for implicit integration\nFixed-point iteration vs. Newton's method\nBackward Euler implementation for ODEs\nConvergence analysis and comparison\nApplication to pendulum dynamics\nPart 1b: Minimization via Newton's Method\nUnconstrained optimization fundamentals\nNewton's method for minimization\nHessian matrix and positive definiteness\nRegularization and line search techniques\nPractical implementation with Julia\nPart 2: Equality Constraints\nLagrange multiplier theory\nKKT conditions for equality constraints\nQuadratic programming with equality constraints\nVisualization of constrained optimization landscapes\nPractical implementation examples\nPart 3: Interior-Point Methods\nInequality constraint handling\nBarrier methods and log-barrier functions\nInterior-point algorithm implementation\nComparison with penalty methods\nConvergence properties and practical considerations","category":"page"},{"location":"class02/overview/#Additional-Resources","page":"Class 2","title":"Additional Resources","text":"","category":"section"},{"location":"class02/overview/","page":"Class 2","title":"Class 2","text":"Lecture Slides (PDF) - Complete slide deck from the presentation \nDemo Script - Python demonstration of penalty vs. barrier methods","category":"page"},{"location":"class02/overview/#Key-Concepts-Covered","page":"Class 2","title":"Key Concepts Covered","text":"","category":"section"},{"location":"class02/overview/#Mathematical-Foundations","page":"Class 2","title":"Mathematical Foundations","text":"","category":"section"},{"location":"class02/overview/","page":"Class 2","title":"Class 2","text":"Gradient and Hessian: Understanding first and second derivatives in optimization\nNewton's Method: Quadratic convergence and implementation details\nKKT Conditions: Necessary and sufficient conditions for optimality\nDuality Theory: Lagrange multipliers and dual problems","category":"page"},{"location":"class02/overview/#Numerical-Methods","page":"Class 2","title":"Numerical Methods","text":"","category":"section"},{"location":"class02/overview/","page":"Class 2","title":"Class 2","text":"Root Finding: Fixed-point iteration, Newton-Raphson method\nImplicit Integration: Backward Euler for stiff ODEs\nSequential Quadratic Programming: Local quadratic approximations\nInterior-Point Methods: Barrier functions and path-following","category":"page"},{"location":"class02/overview/#Constraint-Handling","page":"Class 2","title":"Constraint Handling","text":"","category":"section"},{"location":"class02/overview/","page":"Class 2","title":"Class 2","text":"Equality Constraints: Lagrange multipliers and null-space methods\nInequality Constraints: Active set methods and interior-point approaches\nPenalty Methods: Quadratic and exact penalty functions\nAugmented Lagrangian: Combining penalty and multiplier methods","category":"page"},{"location":"class02/overview/","page":"Class 2","title":"Class 2","text":"","category":"page"},{"location":"class02/overview/","page":"Class 2","title":"Class 2","text":"For questions or clarifications, please reach out to Arnaud Deza at adeza3@gatech.edu","category":"page"},{"location":"class07/StochasticOptimalControl_Overview/#StochasticOptimalControl.jl","page":"StochasticOptimalControl.jl","title":"StochasticOptimalControl.jl","text":"","category":"section"},{"location":"class07/StochasticOptimalControl_Overview/#Overview-of-Stochastic-Optimal-Control-(SOC)","page":"StochasticOptimalControl.jl","title":"Overview of Stochastic Optimal Control (SOC)","text":"","category":"section"},{"location":"class07/StochasticOptimalControl_Overview/#This-file-provides-a-narrative-introduction-to-SOC,-its-motivation,-methods,","page":"StochasticOptimalControl.jl","title":"This file provides a narrative introduction to SOC, its motivation, methods,","text":"","category":"section"},{"location":"class07/StochasticOptimalControl_Overview/#and-applications,-with-formulas-explained-in-context.","page":"StochasticOptimalControl.jl","title":"and applications, with formulas explained in context.","text":"","category":"section"},{"location":"class07/StochasticOptimalControl_Overview/","page":"StochasticOptimalControl.jl","title":"StochasticOptimalControl.jl","text":"println(\"Stochastic Optimal Control chapter loaded. This file can be used to understand and implement LQG, Robust, and Unscented control methods.\")","category":"page"},{"location":"class07/StochasticOptimalControl_Overview/","page":"StochasticOptimalControl.jl","title":"StochasticOptimalControl.jl","text":"println(\"\"\" Stochastic Optimal Control (SOC) is concerned with choosing control actions in systems where both the dynamics and the observations are noisy.  In real-world systems, uncertainties arise from sensor noise, model inaccuracies, and external disturbances.  SOC explicitly accounts for these uncertainties while attempting to optimize a performance objective.","category":"page"},{"location":"class07/StochasticOptimalControl_Overview/","page":"StochasticOptimalControl.jl","title":"StochasticOptimalControl.jl","text":"Consider a discrete-time system with dynamics given by:     x{t+1} = A * xt + B * ut + wt where xt represents the state at time t, ut is the control input, and wt is a stochastic disturbance, typically modeled as a zero-mean Gaussian with covariance Qw. ","category":"page"},{"location":"class07/StochasticOptimalControl_Overview/","page":"StochasticOptimalControl.jl","title":"StochasticOptimalControl.jl","text":"Measurements are also noisy:     yt = C * xt + vt where vt represents measurement noise with covariance R_v. ","category":"page"},{"location":"class07/StochasticOptimalControl_Overview/","page":"StochasticOptimalControl.jl","title":"StochasticOptimalControl.jl","text":"The control objective is usually formulated as the minimization of an expected quadratic cost:     J = E[ sum{t=0}^{T-1} (xt' * Q * xt + ut' * R * u_t) ] Here, Q and R are weighting matrices that balance the importance of state deviations versus control effort.","category":"page"},{"location":"class07/StochasticOptimalControl_Overview/","page":"StochasticOptimalControl.jl","title":"StochasticOptimalControl.jl","text":"This framework allows controllers to explicitly trade off performance and robustness, producing actions that are principled and reliable even under uncertainty.","category":"page"},{"location":"class07/StochasticOptimalControl_Overview/","page":"StochasticOptimalControl.jl","title":"StochasticOptimalControl.jl","text":"SOC has broad applications. In aerospace engineering, it is used to stabilize aircraft under turbulence and wind gusts. In robotics, SOC ensures reliable navigation when sensors are noisy or imperfect. In finance, it helps optimize portfolios under stochastic returns. In process engineering, SOC helps control chemical reactors, distillation columns, and energy systems where measurement noise and disturbances are significant.","category":"page"},{"location":"class07/StochasticOptimalControl_Overview/","page":"StochasticOptimalControl.jl","title":"StochasticOptimalControl.jl","text":"Traditional deterministic control assumes perfect knowledge of the system state and neglects uncertainty. Controllers designed under this assumption often perform poorly or fail when noise or model mismatch is present. SOC remedies this by explicitly modeling uncertainties and incorporating them into the decision-making process.","category":"page"},{"location":"class07/StochasticOptimalControl_Overview/","page":"StochasticOptimalControl.jl","title":"StochasticOptimalControl.jl","text":"Measurement choices directly affect the uncertainties the controller must handle. For example, temperature can be measured using thermocouples, resistance temperature detectors (RTDs), or infrared sensors. Thermocouples are inexpensive and cover a wide temperature range, but they are noisy and less accurate. RTDs are more precise and stable but slower and costlier. Infrared sensors are non-contact and fast, but their readings depend on surface emissivity and line-of-sight. The method chosen determines the type and magnitude of errors the controller will encounter, which influences the design of the control strategy.","category":"page"},{"location":"class07/StochasticOptimalControl_Overview/","page":"StochasticOptimalControl.jl","title":"StochasticOptimalControl.jl","text":"Several stochastic control methods exist, each suited to different types of systems and uncertainty. Linear Quadratic Gaussian (LQG) control is optimal for linear systems with Gaussian noise, providing a separation principle between state estimation and control. Robust control, including H-infinity methods, focuses on worst-case scenarios, ensuring stability and bounded performance under model mismatch or bounded disturbances. Unscented Optimal Control (UOC) and iterative Linear Quadratic Gaussian (iLQG) approaches handle nonlinear dynamics and non-Gaussian noise. Sigma-point propagation, used in UOC, accurately tracks the mean and covariance of the state through nonlinear transformations. iLQG iteratively linearizes the system around a nominal trajectory and computes locally optimal control laws.","category":"page"},{"location":"class07/StochasticOptimalControl_Overview/","page":"StochasticOptimalControl.jl","title":"StochasticOptimalControl.jl","text":"The choice of method depends on the system's linearity, noise characteristics, uncertainty magnitude, and performance versus safety requirements. There is no single stochastic control method that is universally optimal; the system context and design priorities must guide the selection.","category":"page"},{"location":"class07/StochasticOptimalControl_Overview/","page":"StochasticOptimalControl.jl","title":"StochasticOptimalControl.jl","text":"In this chapter, we will examine four major areas. First, LQG control is introduced to illustrate optimal control for linear systems under Gaussian noise and the separation between estimation and control. Second, Kalman filtering is described as a recursive technique for estimating system states from noisy measurements. Third, robust control methods are discussed, contrasting stochastic and worst-case approaches, and introducing H-infinity methods for handling uncertainties and disturbances. Finally, Unscented Optimal Control and iLQG are presented, showing how sigma-point propagation and iterative trajectory optimization allow SOC methods to handle nonlinear stochastic systems effectively.","category":"page"},{"location":"class07/StochasticOptimalControl_Overview/","page":"StochasticOptimalControl.jl","title":"StochasticOptimalControl.jl","text":"This narrative provides the foundation for understanding stochastic optimal control, highlighting the importance of explicitly handling uncertainty and the trade-offs between performance and robustness in practical systems. \"\"\")","category":"page"},{"location":"class10/class10/#Class-10-—-10/24/2025","page":"Class 10","title":"Class 10 — 10/24/2025","text":"","category":"section"},{"location":"class10/class10/","page":"Class 10","title":"Class 10","text":"Presenter: Chris Rackauckas Liason: Michael Klamkin","category":"page"},{"location":"class10/class10/#**Topic:**-Neural-Differential-Equations:-classical-solvers-adjoint-methods","page":"Class 10","title":"Topic: Neural Differential Equations: classical solvers + adjoint methods","text":"","category":"section"},{"location":"class10/class10/","page":"Class 10","title":"Class 10","text":"Chris Rackauckas's Guest Lecture for the Optimal Control & Learning Course.","category":"page"},{"location":"class10/class10/","page":"Class 10","title":"Class 10","text":"The lecture covered Neural Differential Equations: Classical Solvers and Adjoint Methods.","category":"page"},{"location":"class10/class10/","page":"Class 10","title":"Class 10","text":"Video Lecture Link","category":"page"},{"location":"class10/class10/","page":"Class 10","title":"Class 10","text":"Lecture Notes","category":"page"},{"location":"class08/background_materials/README/#Class-8-Background-Material-10/10/2025","page":"Class 8 Background Material - 10/10/2025","title":"Class 8 Background Material - 10/10/2025","text":"","category":"section"},{"location":"class08/background_materials/README/","page":"Class 8 Background Material - 10/10/2025","title":"Class 8 Background Material - 10/10/2025","text":"Lecture structure is still under development. Tentative plan:","category":"page"},{"location":"class08/background_materials/README/#Introduction","page":"Class 8 Background Material - 10/10/2025","title":"Introduction","text":"","category":"section"},{"location":"class08/background_materials/README/","page":"Class 8 Background Material - 10/10/2025","title":"Class 8 Background Material - 10/10/2025","text":"Brief introduction of my research and background.","category":"page"},{"location":"class08/background_materials/README/#Topic-1:-Consensus","page":"Class 8 Background Material - 10/10/2025","title":"Topic 1: Consensus","text":"","category":"section"},{"location":"class08/background_materials/README/","page":"Class 8 Background Material - 10/10/2025","title":"Class 8 Background Material - 10/10/2025","text":"\"Algorithms that provide rapid agreement and teamwork between all participants\" Great foundational paper: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4118472","category":"page"},{"location":"class08/background_materials/README/#Topic-2:-ADMM-(Alternating-Direction-Method-of-Multipliers)","page":"Class 8 Background Material - 10/10/2025","title":"Topic 2: ADMM (Alternating Direction Method of Multipliers)","text":"","category":"section"},{"location":"class08/background_materials/README/","page":"Class 8 Background Material - 10/10/2025","title":"Class 8 Background Material - 10/10/2025","text":"Great foundational textbook: https://web.stanford.edu/~boyd/papers/pdf/admmdistrstats.pdf","category":"page"},{"location":"class08/background_materials/README/","page":"Class 8 Background Material - 10/10/2025","title":"Class 8 Background Material - 10/10/2025","text":"Perhaps introduce with the connection of progressive-hedging (in CEP research) to ADMM?","category":"page"},{"location":"class08/background_materials/README/#Topic-3:-Model-Predictive-Control","page":"Class 8 Background Material - 10/10/2025","title":"Topic 3: Model Predictive Control","text":"","category":"section"},{"location":"class08/background_materials/README/","page":"Class 8 Background Material - 10/10/2025","title":"Class 8 Background Material - 10/10/2025","text":"Foundational paper (tentative?): https://www.sciencedirect.com/science/article/pii/S0005109899002149?","category":"page"},{"location":"class10/background_materials/README/#Class-11-Background-Material-10/31/2025","page":"Class 11 Background Material - 10/31/2025","title":"Class 11 Background Material - 10/31/2025","text":"","category":"section"},{"location":"class01/background_materials/README/#Class-1-Background-Material-08/22/2025","page":"Class 1 Background Material - 08/22/2025","title":"Class 1 Background Material - 08/22/2025","text":"","category":"section"},{"location":"class13/class13/#Class-13-—-11/14/2025","page":"Class 13","title":"Class 13 — 11/14/2025","text":"","category":"section"},{"location":"class13/class13/","page":"Class 13","title":"Class 13","text":"Presenter: Charlelie Laurent Liason: Pedro Paulo Santos","category":"page"},{"location":"class13/class13/","page":"Class 13","title":"Class 13","text":"Topic: Scalable PINNs / neural operators; CFD & weather applications","category":"page"},{"location":"class13/class13/","page":"Class 13","title":"Class 13","text":"","category":"page"},{"location":"class13/class13/","page":"Class 13","title":"Class 13","text":"Charlelie Laurent's Guest Lecture for the Optimal Control & Learning Course.","category":"page"},{"location":"class13/class13/","page":"Class 13","title":"Class 13","text":"The lecture explored the NVIDIA PhysicsNeMo framework, highlighting its use in training AI surrogates to accelerate physics simulations. It also provided a theoretical overview of how Graph Neural Networks can enhance generalization in complex settings.","category":"page"},{"location":"class13/class13/","page":"Class 13","title":"Class 13","text":"Video Lecture Link.","category":"page"},{"location":"class05/class05/#Class-5-—-09/19/2025","page":"Class 5","title":"Class 5 — 09/19/2025","text":"","category":"section"},{"location":"class05/class05/","page":"Class 5","title":"Class 5","text":"Presenter: Guancheng \"Ivan\" Qiu","category":"page"},{"location":"class05/class05/","page":"Class 5","title":"Class 5","text":"Topic: Nonlinear trajectory optimization; collocation; implicit integration; differential dynamic programming","category":"page"},{"location":"class05/class05/","page":"Class 5","title":"Class 5","text":"","category":"page"},{"location":"class05/class05/","page":"Class 5","title":"Class 5","text":"In this lecture, we will cover examples of nonlinear trajectory optimization, and two methods to solve these problems: transcription (specifically collocation) and differential dynamic programming.","category":"page"},{"location":"class05/class05/","page":"Class 5","title":"Class 5","text":"The lecture note is contained in a Pluto notebook (class05/class05.jl). To run it locally, please refer to the Class 01 documentation. (In Step 3: Install Pluto and other dependencies, you should activate the environment in the class05 foler.)","category":"page"},{"location":"class05/class05/","page":"Class 5","title":"Class 5","text":"The notebook can also be accessed online","category":"page"},{"location":"class12/class12/#Class-12-—-11/07/2025","page":"Class 12","title":"Class 12 — 11/07/2025","text":"","category":"section"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"Presenter: Pedro Paulo","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"Topic: Neural operators (FNO, Galerkin Transformer); large-scale surrogates","category":"page"},{"location":"class12/class12/#Foundations-of-Neural-Operators","page":"Class 12","title":"Foundations of Neural Operators","text":"","category":"section"},{"location":"class12/class12/#Notations-and-definitions","page":"Class 12","title":"Notations and definitions","text":"","category":"section"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"Let's start by setting up some notations:","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"Derivatives dfracpartialpartial t are going to be written as partial_t, and, in the case of derivatives w.r.t. time, they could be written as partial_t x=dot x.\nIntegrals int cdot  mathrm dt are going to be written as int mathrm dt  cdot.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"And having some definitions:","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"Vectors are lists of numbers, i.e., a vector v lives in mathbb R^d_v, and can be thought as a list of d_v numbers, all in mathbb R. More generally, vectors could live in a generic vector space V, so we would have vin V. \nFunctions are vector-to-vector mapping, i.e., a function f brings a v in mathbb R^d_v to a w in mathbb R^d_w, and we define that as f mathbb R^d_v rightarrow mathbb R^d_w.  More generally, functions could operate on a generic vector space V and W, so we would have f V rightarrow W. \nOperators are function-to-functions mapping, i.e., an operator A brings an fmathbb R^d_v1 rightarrow mathbb R^d_w1 to a g mathbb R^d_v2 rightarrow mathbb R^d_w2. More generally, operators could operate on generic function spaces, so we would have an operator A bringing an fV_1 rightarrow W_1 to a gV_2 rightarrow W_2. ","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"Key differences:","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"A vector is naturally discrete. Therefore, the input-output pair for functions are also naturally discrete. \nA function is naturally continuous. Therefore, the input-output pair for operators are also naturally continuous.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"It is said that Neural Networks (NN) are universal function approximators [1,2], in this section we're going to create the idea of universal operator approximators, that map functions to functions, using something called Neural Operators.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"A NN mathcal N can be thought as a general function mathcal N X times Theta rightarrow Y, where X and Y are vector spaces, and Theta is the parameter space. So we take elements x in X and we learn how to map those onto yin Y, by means of changing the parameters theta in Theta. That way, we can approximate any function (that's where the \"universal function approximator\" comes from) that maps X rightarrow Y.  In a similar way we can think about a Neural Operator mathcal G^dagger mathcal X times Theta rightarrow mathcal Y, where mathcal X and mathcal Y are function spaces, and Theta is the parameter space. Now, instead of learning how to map vectors, we're going to learn the mapping of functions. This general idea will be expanded further.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"Why are functions important? Everything in the real world is a function! If we want to predict the airflow around a car, the stress caused by deforming a metal bar, the temperature of a reactor, the weather (and the list goes on), we would need to use functions. When putting into a computer we are going to need to mesh our function, otherwise we'd not be able to process it. But we're going to think about functions when designing the architecture of these Neural Operators.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"Why approximate operators? Let's start with a parallel with image processing. Imagine that I have a Convolutional NN (CNN) that take as an input a (discrete) 256times256 image (let's imagine it in grayscale for simplicity). The input to this CNN would then be a v in mathbb R^256 times 256, where each element v_i in mathbb R   v_i in 01. Although this is a typical architecture for image processing [3], and it has been around since 1989 [4], it has a couple of limitations:","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"The input has to be 256times256, the need of different dimension leads to a new NN and a new training.\nIn case of regression, the output has to a fixed dimension, the need of different dimension leads to a new NN and a new training.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"For the case of image processing, where there's no trivial underlying function behind the image, we cannot take advantage of the use of Neural Operators, but in the case of distributions of physical quantities, e.g., temperature, where there's a underlying function behind it, we can leverage the use of Neural Operators to understand distribution function, and make predictions/controls based on it, decoupling the parametrization Theta from the discretization of the data. [5] et al. compared the errors of two networks: U-Net (NN topology) and PCA-Net (Neural operator topology), that were trained on different discretizations of the same underlying function, and the result is shown below:","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"(Image: Alt text)","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"This brings a concept (that we'll try to keep with our definition of Neural Operators) called Discretization Invariance:","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"When we have Discretization Invariance we de-couple the parameters and the cost from the discretization, i.e., when changing the discretization the error doesn't vary.\nIf our model is Discretization Invariable, we can use information at different discretizations to train, and we can transfer parameters learned for one discretization to another, that leads to something called \"zero-shot super-resolution\", that basically consists of training into a smaller discretization and predicting into a bigger one, due to the Discretization Invariance. This concept, together with its limitations, will be discussed in the \"Fourier Neural Operator\" section.","category":"page"},{"location":"class12/class12/#Operator-basics","page":"Class 12","title":"Operator basics","text":"","category":"section"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"Let the operator mathcal G mathcal X rightarrow mathcal Y, where mathcal X and  are separable Banach spaces (mathematical way of saying that mathcal X and mathcal Y are spaces of functions) of vector-valued functions:","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"beginalign\nmathcal X=x Drightarrow mathbb R    D subseteqmathbb R^d\n\nmathcal Y=y Drightarrow mathbb R\nendalign","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"For example, D is a cut plane of a biological tissue (D subseteqmathbb R^2) under the application of electric fields, and xinmathcal X and yinmathcal Y are temperatures before and after the application of said fields. The operator mathcal G is given by:","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"rho c_ppartial_tT =nabla cdot(knabla T) + sigmaE^2-Q","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"where","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"beginalign\nrho text is the tissues density\n\nc_p text is the tissues heat capacity\n\nT text is the temperature distribution on the tissue\n\nk text is the tissues thermal conductivity\n\nsigma text is the tissues electrical conductivity\n\nE text is the electric field distribution\n\nQ text is the heat transfer from bloodmetabolism\nendalign","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"This is one specific case of an operator, but any PDE can be thought as an operator.","category":"page"},{"location":"class12/class12/#Approximations","page":"Class 12","title":"Approximations","text":"","category":"section"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"Imagine that I want to approximate this operator mathcal G by means of an mathcal G^dagger mathcal XtimesTheta rightarrow mathcal Y, a first idea could be to find two linear mappings, here called K_mathcal W and L_mathcal W such that K_mathcal WL_mathcal W approx I, where I is the identity operator (i.e., by applying K_mathcal W and L_mathcal W to all w in mathcal W we will return to the same w), and that by applying K_mathcal W to a winmathcal W we can project this w onto a non-infinite space mathbb R^d_mathcal W (one example of such operator is the FFT family, we're we approximate every function to a finite set of coefficients that represent the original functions' sines and cosines). By doing this to both mathcal X and mathcal Y, we're going to have two non-infinite representations of mathcal X and mathcal Y, on mathbb R^n and mathbb R^m, respectively, and we can map this two representations using a non-linear function varphi. ","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"A general diagram is shown below:","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"(Image: Alt text)","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"In this case, we can see that our mathcal G^dagger can be given by mathcal G^dagger = K_mathcal X circ varphicirc L_mathcal Y, where K_mathcal X and L_mathcal Y are the operators that project mathcal X and mathcal Y to the non-infinite dimension spaces mathbb R^n and mathbb R^n, respectively, and varphi is a non-linear function that maps mathbb R^n to mathbb R^m. Different selections of the set {K_mathcal W, L_mathcal W, varphi} generate different classes of Neural Operators.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"We can, from this, see the first limitation of this technique: we're limited by how well is the approximation of K_mathcal WL_mathcal W approx I. It turns out that, as described by [5], this is approximation is fairly general: Universal approximation: Let:","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"mathcal X\nand mathcal Y be separable Banach spaces.\nmathcal G mathcal X rightarrow mathcal Y\nbe continuous.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"For any Usubset mathcal X compact and epsilon  0, there exists continuous, linear maps K_mathcal Xmathcal X rightarrow mathbb R^n,  L_mathcal Ymathcal Y rightarrow mathbb R^m, and varphi mathbb R^n rightarrow mathbb R^m such that:","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"sup_uin U  mathcal G(u)-mathcal G^dagger(u)_mathcal Y  epsilon","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"Average approximation: Let: ","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"mathcal X\nbe separable Banach spaces, and mu in mathcal P(mathcal X) be a probability measure in mathcal X.\nmathcal G in L_mu^p(mathcal Xmathcal Y)\nfor some 1leq p  infty","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"If mathcal Y is separable Hilbert space, and epsilon  0, there exists continuous, linear maps K_mathcal Xmathcal X rightarrow mathbb R^n,  L_mathcal Ymathcal Y rightarrow mathbb R^m, and varphi mathbb R^n rightarrow mathbb R^m such that:","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":" mathcal G(u)-mathcal G^dagger(u)_L_mu^p(mathcal Xmathcal Y)  epsilon","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"Let's start by giving two classes of Neural Operators, the Principal Component Analysis Network (PCA-NET) and the Deep Operator Network (DeepONet).","category":"page"},{"location":"class12/class12/#PCA","page":"Class 12","title":"PCA","text":"","category":"section"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"First proposed by [6], we're going to define the PCA-NET approximation by analyzing our input and output spaces using a PCA-like technique. Let:","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"mathcal X\nand mathcal Y be separable Banach spaces, and let xin Ksubsetmathcal X, with K compact.\nmathcal G\n(the operator that we're trying to approximate) be continuous.\nvarphi_jmathbb R^n times Theta rightarrow mathbb R^m\nbe multiple neural networks.\nxi_1textxi_n\nbe the PCA basis functions of the input space mathcal X.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"- The operator $K_\\mathcal X$ for a given $x\\in \\mathcal X$ would then be $K_\\mathcal X(x) :=\\mathrm Lx = \\{\\langle\\xi_j,x\\rangle\\}_j$.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"psi_1textpsi_m\nbe the PCA basis functions of the output space mathcal Y.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"The final approximation mathcal G^dagger_textPCAmathcal X times Theta rightarrow mathcal Y is then given by:","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"beginalign\nmathcalG^dagger_textPCA(xtheta)(u)=sum_j=0^mvarphi_j(mathrm Lxtheta)psi_j(u)     forall xinmathcal X      uin D_u\nendalign","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"That is, the output is the linear combination of the PCA output basis functions {psi_j}, weighted by NN coefficients varphi_j, that have as input the mathrm Lx mapping of the input to the PCA space.","category":"page"},{"location":"class12/class12/#DeepONet","page":"Class 12","title":"DeepONet","text":"","category":"section"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"Proposed by [7], the DeepONet generalizes the idea of PCA-NET, by means of learning the PCA basis functions of the output space mathcal Y, i.e., psi_1psi_m are now NNs. The parameter space is then composed of two distinct set of parameters to be learned: theta_varphi, the same parameters as the original PCA-NET, and theta_psi, the parameters for the PCA basis functions of the output space. We will then have:","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"beginalign\nmathcal G^dagger_textDeepONet(xtheta)(u)=sum_j=0^mvarphi_j(mathrm Lxtheta_varphi)psi_j(utheta_psi)     forall xinmathcal X      uin D_u\nendalign","category":"page"},{"location":"class12/class12/#Overcoming-the-curse-of-dimensionality","page":"Class 12","title":"Overcoming the curse of dimensionality","text":"","category":"section"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"One of the big problems of these approaches is the fact L_mathcal Y is a linear combination of the {psi_j}. This leads to the need of an doubly exponential growth in the amount of data, when compared to n (the size of the PCA basis functions of the input space mathcal X), to achieve convergence [8]. To overcome this difficulty, we're going to generalize this idea of linear approximation of operators to the non-linear case.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"Let:","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"mathcal X\nand mathcal Y be function spaces over Omega subset mathbb R^d\nmathcal G^dagger\nis the composition of non-linear operators: mathcal G^dagger=S_1circ text circ S_L ","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"- In the linear case, as described before, $S_1 = K_\\mathcal X$, $S_L = K_\\mathcal Y$ and they're connected through multiple $\\varphi_j$.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"The above definition looks a lot like the typical definition of NNs, where each one of the S_l is a layer of your NN. And, as we're going to see, it is! At least it is a generalization of the definition of NN to function space. [9] proposed to create each one of this S_l as follows:","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"S_l(a)(x) = sigma_lbigg( W_la(x) + b_l + int_Omegamathrm dz  kappa_l(xz)a(z)  bigg)     x in Omega","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"where:","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"sigma_lmathbb R^krightarrowmathbb R^k\nis the non-linear activation function.\nW_linmathbb R^k\nis a term related to a \"residual network\".","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"- This term is not necessary for convergence, but it's credited to help with convergence speed.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"b_linmathbb R^k\nis the bias term.\nkappa_lOmegatimesOmegarightarrowmathbb R^k\nis the kernel function.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"The main distinction between this approach and the traditional NN approach is the kappa_l term, instead of the traditional weights, and the fact that the input a(x) is a function, instead of a vector like the traditional NNs. Different selections of kappa_l generate different classes of these non-linear Neural Operators, but we're going to focus on the transform kappa_l, more specifically the Fourier Neural Operator and the Garlekin Transformer.","category":"page"},{"location":"class12/class12/#Fourier-Neural-Operator","page":"Class 12","title":"Fourier Neural Operator","text":"","category":"section"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"Let kappa_l(xz)=kappa_l(x-z), the integral will then become:","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"int_Omega mathrm dz  kappa_l(xz)a(z) = int_Omega mathrm dz  kappa_l(x-z)a(z) =kappa_l(x) * a(x)","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"where * represents the convolution operator.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"And, as we know from Fourier Transform Theory, ","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"mathcal Fkappa_l(x)*a(x) = mathcal Fkappa_l(x) cdotmathcal Fa(x) =hat kappa_l(v)hat a(v)","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"where mathcal Fcdot represents the Fourier transform of a function.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"We can than reduce the single layer S_l, shown before, to the following:","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"S_l(a)(x) = sigma_lbigg( W_la(x) + b_l + mathcal F^-1hatkappa_l(v) hat a(v)  bigg)     x in Omega     v in Omega^ddagger","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"where Omega^ddagger subset mathbb C^d represent the spectral Fourier space related to Omega.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"This is basically what defines the Fourier Neural Operator (FNO): the Neural Operator mathcal G^dagger=S_1circ text circ S_L where each one of these S_l is done by \"filtering\" the previous output function using its Fourier expansions, first described by [9].","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"The overall diagram of the process is shown bellow, and a walkthrough will follow:","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"(Image: Alt text)","category":"page"},{"location":"class12/class12/#Walkthrough","page":"Class 12","title":"Walkthrough","text":"","category":"section"},{"location":"class12/class12/#Lifting-(P)-and-Projection-(Q)-layers","page":"Class 12","title":"Lifting (P) and Projection (Q) layers","text":"","category":"section"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"The Lifting layer (P) maps the input function from its original low-dimensional channel space into a higher-dimensional latent space. This is typically done with a pointwise linear layer (a 1×1 convolution). The reason for this expansion is that the Fourier layers approximate nonlinear operators more effectively when they operate on a wide latent representation, giving the model the expressive capacity needed to learn complex mappings such as PDE solution operators.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"The Projection layer (Q) performs the opposite transformation: it takes the final high-dimensional latent features produced by the Fourier layers and compresses them back into the desired output channel dimension. Like the lifting layer, it is usually a pointwise linear map. This step converts the latent representation into the actual predicted function (e.g., pressure, velocity, temperature), acting as the final interface between the learned operator and the physical output space.","category":"page"},{"location":"class12/class12/#Fourier-layers","page":"Class 12","title":"Fourier layers","text":"","category":"section"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"As stated before, the Fourier Layers are composed following the equation below:","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"S_l(a)(x) = sigma_lbigg( W_la(x) + b_l + mathcal F^-1hatkappa_l(v) hat a(v)  bigg)     x in Omega     v in Omega^dagger","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"An interesting thing about the the kernel hat kappa_l(v) is that it has a non-zero value for the first couple of values (here called K_kappa) and zero for the last values. That is, the product hatkappa_l(v) cdothat a(v) is given by:","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"(hatkappa_l(v) hat a(v))_j = begincases W_kappahat a_j(v)  jleq K_kappa0  j K_kappa endcases","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"where W_kappa are the (trainable) weights for the kernel, and j represents each mode (\"frequency component\").","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"We can see this \"low-pass filter\" behavior of the kernel represented on the \"zoom\" of the general diagram (b), where the high frequencies vanish, while the remaining low frequencies are multiplied by a certain weight. After this \"filtering\" and weighting, we apply the inverse FFT get the mathcal F^-1hatkappa_l(v) cdothat a(v) term.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"Meanwhile we also have the so called \"residual network\", represented by W_la(x), with trainable W_l. It is not strictly necessary to be used, but it helps with convergence speed, and the (also trainable) bias term b_l, suppressed on the figure. The sum of all the aforementioned terms is then passed by a non-linear activation function sigma, defined a priori.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"And, finally, T (defined a priori) of these layers are concatenated, before being projected down by the layer Q, to produce the output u(x).","category":"page"},{"location":"class12/class12/#Zero-shot-Superresolution","page":"Class 12","title":"Zero-shot Superresolution","text":"","category":"section"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"An interesting fact about the usage of Neural Operators is their Discretization invariance, that is, as shown on Figure 1, a change in discretization didn't lead to a change in test error.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"This was leveraged using FNO to the so-called Zero-shot Superresolution: where the Neural Operator can be trained on a dataset with a smaller discretization (i.e., on a coarser grid) and, using the same Network predict using a finer grid. The following figure showcase this for the Burgers 1D equation, shown below, and with x in mathbb R^256.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"textBurgers 1D equation  fracpartial u t + frac12fracpartial u^2partial x = nu fracpartial^2 upartial x^2","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"(Image: alt text) With the maximum difference between Prediction and Ground Truth being ~ 6e-3. ","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"After the training, the same Network was used to predict outputs for xinmathbb R^2048, and we have the following: (Image: alt text)","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"With the maximum difference between Prediction and Ground Truth being, once again, ~ 6e-3. ","category":"page"},{"location":"class12/class12/#Galerkin-transformer","page":"Class 12","title":"Galerkin transformer","text":"","category":"section"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"An interesting thing about transformer is that, in general, the whole output function depends globally on the input function. I.e., let the function f(x), solution of a certain PDE that has as input g(x), and let x_0inOmega a fixed point; f(x_0) will depend on g(x)forall xinOmega. With this in mind, and creating a parallel with transformers and Attention, Shuhao et al. [10] developed the Galerkin transformer, that uses an \"attention-based\" kernel kappa_l(xz).","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"This kernel embodies the essential non-local coupling across the spatial domain, dictating how information at point z influences the output at point x. In its continuous form, the kernel kappa_l is too complex to parameterize directly. We can achieve an approximation by representing the kernel through a factorized form: kappa_l(x z) approx phi(Q_l a(x))^top psi(K_l a(z)), where Q_l and K_l are learnable linear maps, and phi and psi are feature transformations. Intuitively, each spatial location is mapped to a vector that describes its role in global interactions. ","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"The matrices Q_l and K_l act as projection operators, transforming the local feature a(x) into a query vector q_x = Q_l a(x) and a(z) into a key vector k_z = K_l a(z), respectively. These vectors share a common latent space, and their inner product, q_x cdot k_z, measures the affinity or relevance between the two locations. ","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"To complete the information aggregation, a third linear map, V_l, transforms a(z) into a value vector v_z = V_l a(z). The resulting approximation to the kernel integral int_Omega mathrm dz kappa_l(x z)a(z) is then written as the sum: sum_z phi(Q_l a(x))^top psi(K_l a(z)) v_z. The full discrete neural operator layer thus becomes S_l(a)(x) = sigma_lleft(W_l a(x) + b_l + sum_z phi(Q_l a(x))^top psi(K_l a(z)) V_l a(z)right), where W_l and b_l handle local transformations, and sigma_l introduces nonlinearity. All projection matrices and feature maps are learned, enabling the network to infer the kernel's structure and the relevant latent dynamics.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"The Galerkin transformer is a specific case where the function a(x) is expanded in a finite basis phi_i(x)_i=1^M using a coefficient vector c=(c_1dotsc_M). In this case, attention is computed between these modal coefficients rather than spatial points. Each mode i produces its own query, key, and value vectors via the same projection operators, resulting in the modal update: tildec_i = sigma_lleft(W_l c_i + b_l + sum_j phi(Q_l c_i)^top psi(K_l c_j) V_l c_j right). This modal attention mechanism ensures the learned operator acts within the finite-dimensional Galerkin subspace, preserving the projection structure of PDE solvers while allowing for adaptive, data-driven coupling between modes.","category":"page"},{"location":"class12/class12/#Potential-improvements-and-connection-to-the-PINNs","page":"Class 12","title":"Potential improvements and connection to the PINNs","text":"","category":"section"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"All the networks shown are classified as \"PDE-agnostic\", that is, there's no implicit step that ensures that our predicted output matches the PDE that we're trying to solve. But PINN-based structures are being develop to connect this two concepts [11].","category":"page"},{"location":"class12/class12/#Large-scale-surrogates","page":"Class 12","title":"Large-scale surrogates","text":"","category":"section"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"Traditional FNO applications face a significant limitation when tackling massive, real-world 3D simulations where the input data and network weights cannot fit onto a single GPU. [12] introduced a model-parallel version of FNOs that utilizes domain-decomposition to distribute both the input data and the network weights across multiple GPUs. This innovation allowed the model to handle problems involving billions of variables (e.g., up to 2.6 billion variables on 512 A100 GPUs), making it practical for large-scale applications like simulating multiphase CO₂ dynamics for carbon capture and storage (CCS). By shifting the computational strugle to the training phase, the resulting surrogate model achieved multiple orders of magnitude speedup during inference compared to traditional numerical solvers.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"Another challenge in training deep surrogate models is the storage-intensive process of creating large, high-fidelity datasets. The conventional approach (generating simulations, saving them to disk, and reading them back, commonly named offline training)creates an I/O and storage bottleneck that limits dataset size and diversity. [13] introduced an open-source online training framework designed to suppress this issue. The framework organizes the simultaneous and executes the numerical solvers and a training server in parallel, allowing data to be streamed directly to the network without intermediate disk storage. This file-avoiding method enables training with a potentially limitless amount of unique data, only constrained by available compute resources. By exposing models like FNOs and Fully Connected Networks to significantly larger and more diverse datasets, the framework demonstrated improved model generalization, reducing validation errors and achieving accuracy gains of 16% for FNO and 68% for Fully Connected Networks compared to traditional offline training.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"","category":"page"},{"location":"class12/class12/#References","page":"Class 12","title":"References","text":"","category":"section"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"[1] McCulloch, Warren S., and Walter Pitts. \"A logical calculus of the ideas immanent in nervous activity.\" The bulletin of mathematical biophysics 5.4 (1943): 115-133.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"[2] Chen, Tianping, and Hong Chen. \"Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems.\" IEEE transactions on neural networks 6.4 (1995): 911-917.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"[3] Anwar, Syed Muhammad, et al. \"Medical image analysis using convolutional neural networks: a review.\" Journal of medical systems 42.11 (2018): 226.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"[4] LeCun, Yann, et al. \"Backpropagation applied to handwritten zip code recognition.\" Neural computation 1.4 (1989): 541-551.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"[5] Neural operator: Learning maps between function spaces with applications to pdes.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"[6] Bhattacharya, Kaushik, et al. \"Model reduction and neural networks for parametric PDEs.\" The SMAI journal of computational mathematics 7 (2021): 121-157.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"[7] Lu, Lu, Pengzhan Jin, and George Em Karniadakis. \"Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators.\" arXiv preprint arXiv:1910.03193 (2019).","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"[8] Cohen, Albert, and Ronald DeVore. \"Approximation of high-dimensional parametric PDEs.\" Acta Numerica 24 (2015): 1-159.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"[9] Li, Zongyi, et al. \"Fourier neural operator for parametric partial differential equations.\" arXiv preprint arXiv:2010.08895 (2020).","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"[10] Cao, Shuhao. \"Choose a transformer: Fourier or galerkin.\" Advances in neural information processing systems 34 (2021): 24924-24940.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"[11] Dhingra, Mrigank, et al. \"Localized PCA-Net Neural Operators for Scalable Solution Reconstruction of Elliptic PDEs.\" arXiv preprint arXiv:2509.18110 (2025).","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"[12] Grady, Thomas J., et al. \"Model-parallel Fourier neural operators as learned surrogates for large-scale parametric PDEs.\" Computers & Geosciences 178 (2023): 105402.","category":"page"},{"location":"class12/class12/","page":"Class 12","title":"Class 12","text":"[13] Meyer, Lucas Thibaut, et al. \"Training deep surrogate models with large scale online learning.\" International Conference on Machine Learning. PMLR, 2023.","category":"page"},{"location":"class13/background_materials/README/#Class-13-Background-Material-11/14/2025","page":"Class 13 Background Material - 11/14/2025","title":"Class 13 Background Material - 11/14/2025","text":"","category":"section"},{"location":"class12/background_materials/README/#Class-12-Background-Material-11/07/2025","page":"Class 12 Background Material - 11/07/2025","title":"Class 12 Background Material - 11/07/2025","text":"","category":"section"},{"location":"class15/background_materials/README/#Class-15-Background-Material-10/03/2025","page":"Class 15 Background Material - 10/03/2025","title":"Class 15 Background Material - 10/03/2025","text":"","category":"section"},{"location":"class11/background_materials/README/#Class-10-Background-Material-10/24/2025","page":"Class 10 Background Material - 10/24/2025","title":"Class 10 Background Material - 10/24/2025","text":"","category":"section"},{"location":"class04/background_materials/README/#Class-4-Background-Material-09/12/2025","page":"Class 4 Background Material - 09/12/2025","title":"Class 4 Background Material - 09/12/2025","text":"","category":"section"},{"location":"class08/class08/#Class-8-—-10/10/2025","page":"Class 8","title":"Class 8 — 10/10/2025","text":"","category":"section"},{"location":"class08/class08/","page":"Class 8","title":"Class 8","text":"Presenter: Kevin Wu","category":"page"},{"location":"class08/class08/","page":"Class 8","title":"Class 8","text":"Topic: Consensus, ADMM, and Distributed Optimal Control","category":"page"},{"location":"class08/class08/","page":"Class 8","title":"Class 8","text":"","category":"page"},{"location":"class08/class08/","page":"Class 8","title":"Class 8","text":"In this lecture, we will cover the concept of consensus, ADMM (alternating direction method of multipliers), and how these methods can be applied for distributed optimal control. ","category":"page"},{"location":"class08/class08/","page":"Class 8","title":"Class 8","text":"The lecture note is contained in a Pluto notebook (class08/class08.jl). To run it locally, please refer to the Class 01 documentation. (In Step 3: Install Pluto and other dependencies, you should activate the environment in the class08 folder.)","category":"page"},{"location":"class08/class08/","page":"Class 8","title":"Class 8","text":"The notebook can also be accessed online","category":"page"},{"location":"class01/class01/#Class-1-—-08/22/2025","page":"Class 1 — 08/22/2025","title":"Class 1 — 08/22/2025","text":"","category":"section"},{"location":"class01/class01/","page":"Class 1 — 08/22/2025","title":"Class 1 — 08/22/2025","text":"Presenter: Andrew Rosemberg","category":"page"},{"location":"class01/class01/","page":"Class 1 — 08/22/2025","title":"Class 1 — 08/22/2025","text":"Topic: Course map; why PDE-constrained optimization; tooling overview; stability & state-space dynamics; Lyapunov; discretization issues.","category":"page"},{"location":"class01/class01/","page":"Class 1 — 08/22/2025","title":"Class 1 — 08/22/2025","text":"","category":"page"},{"location":"class01/class01/","page":"Class 1 — 08/22/2025","title":"Class 1 — 08/22/2025","text":"In this first class, we will introduce the course, outlining the topics that will be covered and the importance of PDE-constrained optimization. We discuss the tools that will be used throughout the course and provides an overview of stability and state-space dynamics, including Lyapunov stability. The class also touches on discretization issues that arise in the context of PDEs.","category":"page"},{"location":"class01/class01/#Pre-requisites","page":"Class 1 — 08/22/2025","title":"Pre-requisites","text":"","category":"section"},{"location":"class01/class01/","page":"Class 1 — 08/22/2025","title":"Class 1 — 08/22/2025","text":"Mathematical Background: A solid understanding of calculus and linear algebra is essential. Familiarity with differential equations and optimization techniques is also needed.\nProgramming Skills: Intermediate capacity in a programming language such as Julia, Python, or MATLAB is required for implementing the concepts discussed in class.","category":"page"},{"location":"class01/class01/#Background-Material","page":"Class 1 — 08/22/2025","title":"Background Material","text":"","category":"section"},{"location":"class01/class01/","page":"Class 1 — 08/22/2025","title":"Class 1 — 08/22/2025","text":"A few background materials have been selected to help you prepare for the course. Please review these resources before the first class:","category":"page"},{"location":"class01/class01/#**Git**:","page":"Class 1 — 08/22/2025","title":"Git:","text":"","category":"section"},{"location":"class01/class01/","page":"Class 1 — 08/22/2025","title":"Class 1 — 08/22/2025","text":"Familiarity with Git for version control is recommended. Students should be comfortable with basic Git commands and workflows. Go through the Git Basics guide to get started.","category":"page"},{"location":"class01/class01/#**Julia-and-Pluto**:","page":"Class 1 — 08/22/2025","title":"Julia and Pluto:","text":"","category":"section"},{"location":"class01/class01/","page":"Class 1 — 08/22/2025","title":"Class 1 — 08/22/2025","text":"The course will use Julia for programming assignments and projects. If you are new to Julia, go through either or all of these resources until you feel comfortable with the language:","category":"page"},{"location":"class01/class01/","page":"Class 1 — 08/22/2025","title":"Class 1 — 08/22/2025","text":"Julia for Beginners\nParallel Computing and Scientific Machine Learning (SciML): Methods and Applications\nJuMP Julia Tutorial\nJulia ML Course","category":"page"},{"location":"class01/class01/","page":"Class 1 — 08/22/2025","title":"Class 1 — 08/22/2025","text":"Julia is a high-level, general-purpose dynamic programming language, designed to be fast and productive, for e.g. data science, artificial intelligence, machine learning, modeling and simulation, most commonly used for numerical analysis and computational science.","category":"page"},{"location":"class01/class01/","page":"Class 1 — 08/22/2025","title":"Class 1 — 08/22/2025","text":"Pluto is a programming environment for Julia, designed to be interactive and helpful. Like Jupyter but with more features. Some other Example Notebooks","category":"page"},{"location":"class01/class01/","page":"Class 1 — 08/22/2025","title":"Class 1 — 08/22/2025","text":"A few steps to get started with Julia and Pluto:","category":"page"},{"location":"class01/class01/#Step-1:-Install-Julia","page":"Class 1 — 08/22/2025","title":"Step 1: Install Julia","text":"","category":"section"},{"location":"class01/class01/","page":"Class 1 — 08/22/2025","title":"Class 1 — 08/22/2025","text":"Go to https://julialang.org/downloads and download the current stable release, using the correct version for your operating system (Linux x86, Mac, Windows, etc).","category":"page"},{"location":"class01/class01/#Step-2:-Run-Julia","page":"Class 1 — 08/22/2025","title":"Step 2: Run Julia","text":"","category":"section"},{"location":"class01/class01/","page":"Class 1 — 08/22/2025","title":"Class 1 — 08/22/2025","text":"After installing, make sure that you can run Julia.","category":"page"},{"location":"class01/class01/#Step-3:-Install-Pluto-and-other-dependencies","page":"Class 1 — 08/22/2025","title":"Step 3: Install Pluto and other dependencies","text":"","category":"section"},{"location":"class01/class01/","page":"Class 1 — 08/22/2025","title":"Class 1 — 08/22/2025","text":"Just activate and instantiate the project environment (provided in the class folder):","category":"page"},{"location":"class01/class01/","page":"Class 1 — 08/22/2025","title":"Class 1 — 08/22/2025","text":"using Pkg\nPkg.activate(\"path/to/Class01_Folder\")","category":"page"},{"location":"class01/class01/","page":"Class 1 — 08/22/2025","title":"Class 1 — 08/22/2025","text":"Project.toml and Manifest.toml files are provided in the class folder, which will install all the necessary packages for this course at the correct versions.","category":"page"},{"location":"class01/class01/#Step-4:-Start-Pluto","page":"Class 1 — 08/22/2025","title":"Step 4: Start Pluto","text":"","category":"section"},{"location":"class01/class01/","page":"Class 1 — 08/22/2025","title":"Class 1 — 08/22/2025","text":"julia> using Pluto\n\njulia> Pluto.run()","category":"page"},{"location":"class01/class01/#Step-5:-Opening-an-existing-notebook-file","page":"Class 1 — 08/22/2025","title":"Step 5: Opening an existing notebook file","text":"","category":"section"},{"location":"class01/class01/","page":"Class 1 — 08/22/2025","title":"Class 1 — 08/22/2025","text":"To run a local notebook file that you have not opened before, then you need to enter its full path (e.g. path/to/math_basics.jl) into the blue box in the main menu.","category":"page"},{"location":"class01/class01/#**Linear-Algebra**:","page":"Class 1 — 08/22/2025","title":"Linear Algebra:","text":"","category":"section"},{"location":"class01/class01/","page":"Class 1 — 08/22/2025","title":"Class 1 — 08/22/2025","text":"We have prepared a basic (Pluto) Linear Algebra Primer to help you brush up on essential concepts. This primer covers key topics such as matrix operations, eigenvalues, and eigenvectors besides other fundamental calculus concepts. It is recommended to review this primer before the first class.","category":"page"},{"location":"class01/class01/#**Optimization**:","page":"Class 1 — 08/22/2025","title":"Optimization:","text":"","category":"section"},{"location":"class01/class01/","page":"Class 1 — 08/22/2025","title":"Class 1 — 08/22/2025","text":"We will use JuMP for some optimization tasks. If you are new to JuMP, please review the JuMP Tutorial to familiarize yourself with its syntax and capabilities.","category":"page"},{"location":"class01/class01/","page":"Class 1 — 08/22/2025","title":"Class 1 — 08/22/2025","text":"Test your knowledge with this (Pluto) Modeling Exercise.","category":"page"},{"location":"class01/class01/","page":"Class 1 — 08/22/2025","title":"Class 1 — 08/22/2025","text":"Final 🧠: The (Pluto) Motivational Exercise will test what you have learned and motivate what we will research in the course.","category":"page"},{"location":"class01/class01/#In-Class-Material","page":"Class 1 — 08/22/2025","title":"In-Class Material","text":"","category":"section"},{"location":"class01/class01/","page":"Class 1 — 08/22/2025","title":"Class 1 — 08/22/2025","text":"Besides the administrative topics, we will cover the structure of the problem we are trying to solve and start with the basics of how to model it. The main (Pluto) Class 01 Notebook contains the in-class material.","category":"page"},{"location":"class07/background_materials/README/#Class-7-Background-Material-10/03/2025","page":"Class 7 Background Material - 10/03/2025","title":"Class 7 Background Material - 10/03/2025","text":"","category":"section"},{"location":"class05/background_materials/README/#Class-5-Background-Material-09/19/2025","page":"Class 5 Background Material - 09/19/2025","title":"Class 5 Background Material - 09/19/2025","text":"","category":"section"},{"location":"class03/background_materials/README/#Class-3-Background-Material-09/05/2025","page":"Class 3 Background Material - 09/05/2025","title":"Class 3 Background Material - 09/05/2025","text":"","category":"section"},{"location":"class06/class06/#Class-6-—-09/26/2025","page":"Class 6","title":"Class 6 — 09/26/2025","text":"","category":"section"},{"location":"class06/class06/","page":"Class 6","title":"Class 6","text":"Presenter: Henrique Ferrolho Liason: Andrew Rosemberg","category":"page"},{"location":"class06/class06/","page":"Class 6","title":"Class 6","text":"Topic: Trajectory optimization on robots in Julia Robotics","category":"page"},{"location":"class06/class06/","page":"Class 6","title":"Class 6","text":"","category":"page"},{"location":"class06/class06/","page":"Class 6","title":"Class 6","text":"Henrique Ferrolho's Guest Lecture for the Optimal Control & Learning Course.","category":"page"},{"location":"class06/class06/","page":"Class 6","title":"Class 6","text":"The lecture covered Nonlinear trajectory optimization for robots in Julia.","category":"page"},{"location":"class06/class06/","page":"Class 6","title":"Class 6","text":"Video Lecture Link","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"<!– Git Adventure Guide –>","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Author: Andrew Rosemberg","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Date: 2025-08-06","category":"page"},{"location":"class01/background_materials/git_adventure_guide/#Git-Adventure-Guide","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"","category":"section"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"A quirky journey from zero to hero with Git","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Git is like a time machine for your code, a collaboration wizard, and a safety net all rolled into one. This guide will take you through the magical world of Git with fun examples, tips, and mini quests to level up your skills.","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"                               /\\                /\\                /\\\n                              /  \\              /  \\              /  \\\n                             / /\\ \\            / /\\ \\            / /\\ \\\n                            / /  \\ \\          / /  \\ \\          / /  \\ \\\n                           / /____\\ \\        / /____\\ \\        / /____\\ \\\n                          /_/      \\_\\      /_/      \\_\\      /_/      \\_\\\n  ___________________________________________________________________________\n /                                                                           \\\n|    ____ _ _       _     _                _____                              |\n|   / ___(_) |_ ___| |__ (_)_ __   __ _   / ____|                             |\n|  | |  _| | __/ __| '_ \\| | '_ \\ / _` | | |  __  ___ _ __                    |\n|  | |_| | | || (__| | | | | | | | (_| | | | |_ |/ _ \\ '__|                   |\n|   \\____|_|\\__\\___|_| |_|_|_| |_|\\__, | | |__| |  __/ |                      |\n|                                |___/   \\_____|\\___|_|                      |\n|                                                                             |\n|                  ~ ~ ~  G I T   A D V E N T U R E  ~ ~ ~                    |\n \\___________________________________________________________________________/\n                          \\      /            \\      /            \\      /\n                           \\____/              \\____/              \\____/","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Git is a version control system that helps you track changes in your code, collaborate with others, and manage different versions of your projects. Whether you're a solo developer or part of a team, mastering Git is essential for modern software development.","category":"page"},{"location":"class01/background_materials/git_adventure_guide/#Table-of-Contents","page":"🚀 Git Adventure Guide","title":"📜 Table of Contents","text":"","category":"section"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"🌱 Getting Started\n📚 Core Concepts\n🪄 Everyday Workflow\n🌳 Branching & Merging\n🤝 Collaborating on GitHub\n🧹 Undo & Fix\n🛠️ Advanced Magic\n🎮 Mini Quests\n🧾 Cheat Sheet\n🏁 Next Steps","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"","category":"page"},{"location":"class01/background_materials/git_adventure_guide/#Getting-Started","page":"🚀 Git Adventure Guide","title":"🌱 Getting Started","text":"","category":"section"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Install Git","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Git is open-source and available on all major platforms. Here’s how to get it:","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"# macOS\nbrew install git\n\n# Debian/Ubuntu\nsudo apt install git\n\n# Windows\nscoop install git","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Set Your Identity","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Versions need an author! Configure your name and email so Git knows who you are:","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"git config --global user.name \"Your Name\"\ngit config --global user.email \"you@example.com\"","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"🧩 Puzzle: Run git config --list — how many settings can you spot?","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"","category":"page"},{"location":"class01/background_materials/git_adventure_guide/#Core-Concepts","page":"🚀 Git Adventure Guide","title":"📚 Core Concepts","text":"","category":"section"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Concept What It Means Emoji Memory Hack\nRepository (Repo) A project folder under Git’s watch 🗂️\nCommit A snapshot of your code 📸\nBranch A timeline of commits 🌿\nMerge Combine branches 🔀\nRemote A repository living elsewhere 🌐","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"🤓 Did you know? Commits are immutable. They’re like fossils—once created, they never change!","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"","category":"page"},{"location":"class01/background_materials/git_adventure_guide/#Everyday-Workflow","page":"🚀 Git Adventure Guide","title":"🪄 Everyday Workflow","text":"","category":"section"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Initialize a new repository  ","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"If you’re starting a new project, create a new Git repository:","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"git init my-awesome-project\ncd my-awesome-project","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"🧩 Puzzle: What happens if you run git init in a directory that already has a .git folder?","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"🏁 Your Journey Starts: Initialize a new repository.","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Clone a repo to your local machine","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"You can create a repository directly on GitHub or any other Git hosting service, which will give you a URL to clone it.","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Once you have a remote repository, you can clone it to your local machine:","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"git clone <url>","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"This process will also save the remote repository URL to a variable called origin, which is the default name for the remote repository. A repository remote is a version of your repository that is hosted on the internet or another network, allowing you to collaborate with others.","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"🧩 Puzzle: Can you have multiple remote copies' URLs stored in your local git configuration?","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"🏗️ Quest Preparation: Create a repository on Github. Then, since we started our repository locally, add the URL manually: ","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"git remote add origin <url>","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Hack away  ","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Changes can be bundled or committed separately. Staging is the process of preparing files for a commit.  You can stage specific files or all changes:","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"git add <files>      # stage\ngit commit -m \"feat: add lunar lasers\"  # snapshot","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Commits (versions) will store your changes with a message describing what you did.","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"🧩 Puzzle: How do you stage all current changes at once?","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"🏗️ First Quest: Create a few files and make your first commit(s)","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Push your masterpiece to the remote repository:","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"In order to collaborate with others, you need to push your changes to the remote repository:","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"git push origin main","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"🌟 Pro‑tip: Use git add -p to stage hunks interactively like a choose‑your‑own‑adventure book.","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"🏗️ Second Quest: After making a few commits, then push them to the remote repository.","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"","category":"page"},{"location":"class01/background_materials/git_adventure_guide/#Branching-and-Merging","page":"🚀 Git Adventure Guide","title":"🌳 Branching & Merging","text":"","category":"section"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"A branch is a separate line of development. It allows you to work on features or fixes without affecting the main codebase. This is especially useful for experimenting or developing new features and avoiding breaking the main code besides avoiding conflicts with other developers' work. Create and jump to a branch:","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"git switch -c <branch-name>","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"🧩 Pro-tip: Name your branch something that helps others infer the author and what you are developing without them needing to look at the meta information of your commits. For example: ar/feature/gpu-compatibility, ar/bugfix/memory-leak, or ar/hotfix/security-patch.","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Merge it back - move your changes into the main branch:","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"git switch main\ngit merge feature/space-pizza","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"If others have made changes to the main branch, you might need to resolve conflicts. The merge command you move you to a temporary state where you can resolve conflicts before finalizing the merge. Git will tell you which files are in conflict by marking them in the output. You can also see the conflicts by running:","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"git status","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Resolve conflicts like a diplomat—edit the conflicted files, then:","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"git add .\ngit commit -m \"fix: resolve cosmic cheese conflict\"","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"⚔️ Boss Battle: Create two conflicting branches and practice the merge dance.","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"","category":"page"},{"location":"class01/background_materials/git_adventure_guide/#Collaborating-on-GitHub","page":"🚀 Git Adventure Guide","title":"🤝 Collaborating on GitHub","text":"","category":"section"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Fork ⇒ get your own copy of a repository to work on.","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Click the \"Fork\" button on the top right of the repo page.","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"🧩 Puzzle: What’s the difference between forking and cloning?","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"🏗️ Third Quest: Fork the repository of this class.","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Clone your fork.","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"In order to work on your forked repository, you need to clone it to your local machine:","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"git clone <url-of-your-fork>\ncd <your-fork-directory>","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Sync with upstream.  ","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"If the original repository has new changes, you can keep your fork up to date by syncing it with the upstream repository:","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"git remote add upstream <url-of-original>\ngit fetch upstream\ngit rebase upstream/main","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"However, this is much easier on the GitHub website.  ","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Go to your forked repository on GitHub and sync it with the original repository by clicking the \"Sync fork\" button.","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Push & create a Pull Request.","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Pull Requests (PRs) are how you propose changes to a project. After pushing your changes to your fork, go to the original repository and click \"New Pull Request\".","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"🧩 Puzzle: What’s the difference between a Pull Request and a Merge?","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"🏗️ Fourth Quest: Make a change in your fork, push it, and create a Pull Request to the original repository. Do this to add your name to your chosen Lecture (Both in the Readme file and in the class folder markdown file) and/or correct any of my many spelling mistakes.","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Celebrate with GIFs. 🎉","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"🏆 End: End of your first adventure. The rest is important to know so read it for the in-class test, but I will let you practice once it makes sense in your daily workflow.","category":"page"},{"location":"class01/background_materials/git_adventure_guide/#Undo-and-Fix","page":"🚀 Git Adventure Guide","title":"🧹 Undo & Fix","text":"","category":"section"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Git is a powerful tool, but sometimes you make mistakes. Here are some common scenarios and how to fix them:","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Situation Command Explanation\nUn‑stage a file git restore --staged file Put it back in working area\nAmend last commit git commit --amend Edit message or add files\nTime‑travel (soft) git reset --soft HEAD~1 Keep changes staged\nTime‑travel (hard) git reset --hard HEAD~1 WARNING: destroys work\nDeleted a branch too soon git reflog Find the commit hash & resurrect","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"","category":"page"},{"location":"class01/background_materials/git_adventure_guide/#Advanced-Magic","page":"🚀 Git Adventure Guide","title":"🛠️ Advanced Magic","text":"","category":"section"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Stash spells  ","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"git stash push -m \"WIP dragon taming\"\ngit stash list\ngit stash pop","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Bisect detective  ","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"git bisect start\ngit bisect bad          # current commit is broken\ngit bisect good v2.0.0  # last known good tag","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Git walks the commit tree to find the culprit!","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Cherry‑pick delicacy  ","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"git cherry-pick <hash>","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"","category":"page"},{"location":"class01/background_materials/git_adventure_guide/#Cheat-Sheet","page":"🚀 Git Adventure Guide","title":"🧾 Cheat Sheet","text":"","category":"section"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"# add everything\ngit add -A\n\n# show history graph\ngit log --oneline --graph --all --decorate\n\n# rename a branch\ngit branch -m old-name new-name\n\n# delete local & remote branch\ngit branch -d feature\ngit push origin --delete feature","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"","category":"page"},{"location":"class01/background_materials/git_adventure_guide/#Next-Steps","page":"🚀 Git Adventure Guide","title":"🏁 Next Steps","text":"","category":"section"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"Watch Oh My Git! interactive game.\nFollow the legendary Pro Git book.\nExplore GitHub Actions for automation.","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"✉️ Send feedback: open an issue or PR—Git isn’t just code; it’s conversation!","category":"page"},{"location":"class01/background_materials/git_adventure_guide/","page":"🚀 Git Adventure Guide","title":"🚀 Git Adventure Guide","text":"","category":"page"},{"location":"class01/background_materials/git_adventure_guide/#2025-Git-Adventure-Guide","page":"🚀 Git Adventure Guide","title":"© 2025 Git Adventure Guide","text":"","category":"section"},{"location":"class03/class03/#Class-3-—-09/05/2025","page":"Class 3","title":"Class 3 — 09/05/2025","text":"","category":"section"},{"location":"class03/class03/","page":"Class 3","title":"Class 3","text":"Presenter: Zaowei Dai","category":"page"},{"location":"class03/class03/","page":"Class 3","title":"Class 3","text":"Topic: Pontryagin’s Maximum Principle; shooting & multiple shooting; LQR, Riccati, QP viewpoint (finite / infinite horizon)","category":"page"},{"location":"class03/class03/","page":"Class 3","title":"Class 3","text":"","category":"page"},{"location":"class03/class03/","page":"Class 3","title":"Class 3","text":"Theory content is available here","category":"page"},{"location":"class03/class03/","page":"Class 3","title":"Class 3","text":"The notebook for this class is available here.","category":"page"},{"location":"class03/class03/","page":"Class 3","title":"Class 3","text":"Theory slides are available here.","category":"page"},{"location":"class09/background_materials/README/#Class-9-Background-Material-10/17/2025","page":"Class 9 Background Material - 10/17/2025","title":"Class 9 Background Material - 10/17/2025","text":"","category":"section"},{"location":"class11/class11/#Class-11-—-10/31/2025","page":"Class 11 — 10/31/2025","title":"Class 11 — 10/31/2025","text":"","category":"section"},{"location":"class11/class11/","page":"Class 11 — 10/31/2025","title":"Class 11 — 10/31/2025","text":"Presenter: Michael Klamkin","category":"page"},{"location":"class11/class11/","page":"Class 11 — 10/31/2025","title":"Class 11 — 10/31/2025","text":"Topic: Physics-Informed Neural Networks (PINNs): formulation & pitfalls","category":"page"},{"location":"class11/class11/","page":"Class 11 — 10/31/2025","title":"Class 11 — 10/31/2025","text":"","category":"page"},{"location":"class11/class11/","page":"Class 11 — 10/31/2025","title":"Class 11 — 10/31/2025","text":"Add notes, links, and resources below.","category":"page"},{"location":"class04/class04/#Class-4-—-09/12/2025","page":"Class 4","title":"Class 4 — 09/12/2025","text":"","category":"section"},{"location":"class04/class04/","page":"Class 4","title":"Class 4","text":"Presenter: Joaquim Dias Garcia Liason: Andrew Rosemberg","category":"page"},{"location":"class04/class04/","page":"Class 4","title":"Class 4","text":"Topic: Dynamic Programming & Model-Predictive Control","category":"page"},{"location":"class04/class04/","page":"Class 4","title":"Class 4","text":"","category":"page"},{"location":"class04/class04/","page":"Class 4","title":"Class 4","text":"Joaquim Dias Garcia's Guest Lecture for the Optimal Control & Learning Course.","category":"page"},{"location":"class04/class04/","page":"Class 4","title":"Class 4","text":"The lecture covered Multistage Stochastic Programming and Stochastic Dual Dynamic Programming (SDDP).","category":"page"},{"location":"class04/class04/","page":"Class 4","title":"Class 4","text":"Video Lecture Link","category":"page"},{"location":"class15/class15/#Class-15-—-10/03/2025","page":"Class 15","title":"Class 15 — 10/03/2025","text":"","category":"section"},{"location":"class15/class15/","page":"Class 15","title":"Class 15","text":"Presenter: Shuaicheng (Allen) Tong","category":"page"},{"location":"class15/class15/","page":"Class 15","title":"Class 15","text":"Topic: Dynamic Optimal Control of Power Systems; Generators swing equations, Transmission lines electromagnetic transients, dynamic load models, and inverters.","category":"page"},{"location":"class15/class15/","page":"Class 15","title":"Class 15","text":"","category":"page"},{"location":"class15/class15/#Overview","page":"Class 15","title":"Overview","text":"","category":"section"},{"location":"class15/class15/","page":"Class 15","title":"Class 15","text":"This chapter introduces the foundational dynamic behaviors of electric power systems and shows how they are incorporated into modern optimal control formulations such as Transient Stability–Constrained Optimal Power Flow (TSC-OPF). We begin with the physics of electromagnetic transients to motivate the formulation of TSC-OPF, move through generator and inverter dynamics, and conclude with dynamic load models that capture how demand responds during disturbances. Together, these components form the backbone required to understand, simulate, and optimize real-world power system behavior.","category":"page"},{"location":"class15/class15/#Materials","page":"Class 15","title":"Materials","text":"","category":"section"},{"location":"class15/class15/","page":"Class 15","title":"Class 15","text":"The chapter is implemented as a Pluto notebook, which contains derivations, visuals, and algorhtmic examples. The lecture slide contains the material of the video recording.","category":"page"},{"location":"class15/class15/#Topics-Covered","page":"Class 15","title":"Topics Covered","text":"","category":"section"},{"location":"class15/class15/#Transients-and-Electromagnetic-Dynamics","page":"Class 15","title":"Transients and Electromagnetic Dynamics","text":"","category":"section"},{"location":"class15/class15/","page":"Class 15","title":"Class 15","text":"Physical origin of transients in power system \nConnection between Faraday's law, inductors/capacitors, and transient behavior  \nRelation of time-domain differential equations to steady-state phasor models  \nIntroduction to transmission-line dynamics and telegrapher’s equations  ","category":"page"},{"location":"class15/class15/#Generator-Swing-Equation","page":"Class 15","title":"Generator Swing Equation","text":"","category":"section"},{"location":"class15/class15/","page":"Class 15","title":"Class 15","text":"Rotor acceleration and deceleration under power imbalance  \nRole of inertia in stabilizing frequency  \nPer-unit formulation and damping effects  ","category":"page"},{"location":"class15/class15/#Inverter-Dynamics-and-Grid-Control","page":"Class 15","title":"Inverter Dynamics and Grid Control","text":"","category":"section"},{"location":"class15/class15/","page":"Class 15","title":"Class 15","text":"Differences between synchronous generators and renewable inverters  \nGrid-following vs. grid-forming behavior  \nVirtual inertia and frequency droop control for renewable integration  ","category":"page"},{"location":"class15/class15/#Dynamic-Load-Models","page":"Class 15","title":"Dynamic Load Models","text":"","category":"section"},{"location":"class15/class15/","page":"Class 15","title":"Class 15","text":"Limitations of static OPF load representations  \nInduction motor dynamics: slip, torque imbalance, and stalling behavior  \nVoltage recovery models such as Exponential Recovery Load (ERL)  \nDifferences between physics-based motor models and empirical aggregate models  ","category":"page"},{"location":"class15/class15/#Transient-Stability–Constrained-Optimal-Power-Flow-(TSC-OPF)","page":"Class 15","title":"Transient Stability–Constrained Optimal Power Flow (TSC-OPF)","text":"","category":"section"},{"location":"class15/class15/","page":"Class 15","title":"Class 15","text":"Time-domain constraints ensuring system stability during disturbances  \nSolution methods such as direct transcription and multiple-shooting formulations  \nForward and adjoint sensitivity analysis for efficient gradient calculation","category":"page"},{"location":"class15/class15/#Learning-Objectives","page":"Class 15","title":"Learning Objectives","text":"","category":"section"},{"location":"class15/class15/","page":"Class 15","title":"Class 15","text":"By the end of this chapter, readers will be able to:","category":"page"},{"location":"class15/class15/","page":"Class 15","title":"Class 15","text":"Describe the physical origins of transients and how they propagate in networks  \nExplain generator swing dynamics and how they regulate grid frequency  \nUnderstand how inverter controls emulate generator behavior for stability  \nDistinguish between static and dynamic load models and when each is appropriate  \nInterpret how system dynamics are embedded in TSC-OPF formulations  \nUnderstand the role of sensitivity analysis in dynamic optimal control","category":"page"},{"location":"class06/background_materials/README/#Class-6-Background-Material-09/26/2025","page":"Class 6 Background Material - 09/26/2025","title":"Class 6 Background Material - 09/26/2025","text":"","category":"section"},{"location":"class07/class07/#Class-7-—-10/03/2025","page":"Class 7","title":"Class 7 — 10/03/2025","text":"","category":"section"},{"location":"class07/class07/","page":"Class 7","title":"Class 7","text":"Presenter: Jouke van Westrenen","category":"page"},{"location":"class07/class07/","page":"Class 7","title":"Class 7","text":"Topic: Essentials of PDEs for control engineers; weak forms; FEM/FDM review","category":"page"},{"location":"class07/class07/","page":"Class 7","title":"Class 7","text":"","category":"page"},{"location":"class07/class07/","page":"Class 7","title":"Class 7","text":"Files for this class are available here.","category":"page"},{"location":"#Special-Topics-on-Optimal-Control-and-Learning-—-Fall-2025-(ISYE-8803-VAN)","page":"Home","title":"Special Topics on Optimal Control and Learning — Fall 2025 (ISYE 8803 VAN)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Georgia Institute of Technology – Fridays 2 pm ET","category":"page"},{"location":"","page":"Home","title":"Home","text":"Designers: Andrew Rosemberg & Michael Klamkin","category":"page"},{"location":"","page":"Home","title":"Home","text":"Instructor: Prof. Pascal Van Hentenryck","category":"page"},{"location":"","page":"Home","title":"Home","text":"2025 cohort: TBD","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This student-led course explores modern techniques for controlling — and learning to control — dynamical systems. Topics range from classical optimal control and numerical optimization to reinforcement learning, PDE-constrained optimization (finite-element methods, Neural DiffEq, PINNs, neural operators), and GPU-accelerated workflows.","category":"page"},{"location":"#Prerequisites","page":"Home","title":"Prerequisites","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Solid linear-algebra background  \nProgramming experience in Julia, Python, or MATLAB  \nBasic ODE familiarity","category":"page"},{"location":"#Index","page":"Home","title":"Index","text":"","category":"section"},{"location":"class09/class09/#Class-9-—-10/17/2025","page":"Class 9","title":"Class 9 — 10/17/2025","text":"","category":"section"},{"location":"class09/class09/","page":"Class 9","title":"Class 9","text":"Presenter: François Pacaud Liason: Arnaud Deza","category":"page"},{"location":"class09/class09/","page":"Class 9","title":"Class 9","text":"Topic: GPU-accelerated optimal control.","category":"page"},{"location":"class09/class09/","page":"Class 9","title":"Class 9","text":"","category":"page"},{"location":"class09/class09/","page":"Class 9","title":"Class 9","text":"François Pacaud's Guest Lecture for the Optimal Control & Learning Course.","category":"page"},{"location":"class09/class09/","page":"Class 9","title":"Class 9","text":"The lecture covered Nonlinear Programming theory, the fundamentals and practical lessons of GPU-accelerated optimization, and applications in optimal control.","category":"page"},{"location":"class09/class09/","page":"Class 9","title":"Class 9","text":"Video Lecture Link","category":"page"}]
}
